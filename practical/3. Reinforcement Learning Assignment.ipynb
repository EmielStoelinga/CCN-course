{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this practical assignment you will learn how to work with reinforcement learning. You will be provided with an environment which generates rewards. It is your goal to implement a Tabular q-learning class which is able to accumulate reward after training.\n",
    "\n",
    "Your code will be evaluated on clarity, conciseness and efficiency. Document your code. The resulting notebook including the required outputs should be converted to pdf and uploaded to Blackboard. Comments can be added in the notebook using markdown cells. Note that for code development you may want to use the PyCharm IDE, which facilitates debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "Below we provide a class Foo, which is an environment with which an agent can interact. Use a markdown cell to explain how the environment generates observations and responds to actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Foo(object):\n",
    "    \"\"\"\n",
    "    Very simple environment for testing fully observed models. The actor gets a reward when it correctly decides\n",
    "    on the ground truth. Ground truth 0/1 determines probabilistically the number of 0s or 1s on the output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, p = 0.8):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            n: number of inputs\n",
    "            p: probability of emitting the right sensation at the input\n",
    "        \"\"\"\n",
    "\n",
    "        super(Foo, self).__init__()\n",
    "\n",
    "        self.ninput = n\n",
    "        self.p = p\n",
    "        self.noutput = 2\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.state = np.random.randint(0, 2)\n",
    "\n",
    "        p = np.array([1 - self.p, self.p])\n",
    "\n",
    "        if self.state == 0:\n",
    "            p = 1 - p\n",
    "\n",
    "        obs = np.random.choice(2, [1, self.ninput], True, p)\n",
    "\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # reward is +1 or -1\n",
    "        reward = 2 * int(action == self.state) - 1\n",
    "\n",
    "        obs = self.reset()\n",
    "        done = True\n",
    "\n",
    "        return obs, reward, done\n",
    "\n",
    "    def get_ground_truth(self):\n",
    "        \"\"\"\n",
    "        Returns: ground truth state of the environment\n",
    "        \"\"\"\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def set_ground_truth(self, ground_truth):\n",
    "        \"\"\"\n",
    "        :param: ground_truth : sets ground truth state of the environment\n",
    "        \"\"\"\n",
    "\n",
    "        self.state = ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "\n",
    "Below we provide a default implementation of an agent which takes random actions and an experimental run which shows how an agent can interact with an environment. Run the random agent on the environment and plot the cumulative reward gained throughout the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Random agent\n",
    "\n",
    "class RandomAgent(object):\n",
    "    \"\"\"\n",
    "    Agent which takes random actions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ninput, noutput, **kwargs):\n",
    "\n",
    "        # Number of input variables\n",
    "        self.ninput = ninput\n",
    "\n",
    "        # Number of actions\n",
    "        self.noutput = noutput\n",
    "\n",
    "    def act(self, obs):\n",
    "        \"\"\"\"\n",
    "        Perform random action.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.random.randint(self.noutput)\n",
    "    \n",
    "    def learn(self,obs,obs2,action,reward,done):\n",
    "        \"\"\"\n",
    "        Nothing to do\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of iterations\n",
    "niter = 10**3\n",
    "\n",
    "###########\n",
    "# Environment specification\n",
    "\n",
    "env = Foo(2,0.8)\n",
    "\n",
    "###########\n",
    "# Agent specification\n",
    "\n",
    "agent = RandomAgent(env.ninput, env.noutput)\n",
    "\n",
    "###########\n",
    "# train phase\n",
    "\n",
    "rewards = np.zeros([niter, 1])\n",
    "\n",
    "obs = env.reset()\n",
    "reward = done = None\n",
    "\n",
    "for i in xrange(niter):\n",
    "\n",
    "    # Choose an action\n",
    "    action = agent.act(obs)\n",
    "\n",
    "    # Perform action and receive new observations and reward\n",
    "    obs2, reward, done = env.step(action)\n",
    "\n",
    "    # Store reward\n",
    "    rewards[i] = reward\n",
    "    \n",
    "    # Tabular q learning\n",
    "    agent.learn(obs,obs2,action,reward,done)\n",
    "    \n",
    "    obs = obs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "\n",
    "Now implement a TabularQAgent which changes the policy using tabular q-learning. Reuse the above experimental run to show that the cumulative reward increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4\n",
    "\n",
    "Show how playing around with the observation probability <code>p</code> affects convergence."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
